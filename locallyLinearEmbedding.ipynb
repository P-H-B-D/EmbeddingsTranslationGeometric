{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOqEzZCT5q_W"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Local Linearization Accuracy to find the spikiness of the data. However, due to the fact that Euclidean distances are used in such high dimensions, the metric proved to be unreliable to find out gemoetric data. By nature of the fact that this heuristic is sensitive to the exact topology of the data, we concluded that applying this method after dimensionality reduction would end in a loss of context and thus invalid measurements."
      ],
      "metadata": {
        "id": "Xpdna5iemOkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Our objective is to somehow concretely capture spikiness of data via some nice geometric benchmark. Measuring curvature directly is\n",
        "highly nontrivial both in terms of theory and computation -- determinants of Riemannian tensors(?) -- so I wouldn't risk spending much time on it. Here's a nice heuristic to replace\n",
        "it. In the sklearn library there is a method called LocallyLinearEmbedding -- look it up.\n",
        "Take the weight matrices calculated by LLE, and compute the error -- high error indicates that some vector v_i doesn't lie on the \"closest\" subspace. This means that there is high \"spikiness\" at v_i wrt its knn.\n",
        "We can iterate this test over subsamples of the set of vectors.\n",
        "\"\"\"\n",
        "\n",
        "# Computes the weights of LLA\n",
        "def compute_weights(X,d_size, d_dim, n_neig):\n",
        "  knn = NearestNeighbors(n_neighbors = n_neig)\n",
        "  knn.fit(X)\n",
        "  distances, indices = knn.kneighbors(X)\n",
        "  W = np.zeros((d_size, d_size))\n",
        "  for i in range(d_size):\n",
        "      M = X[indices[i]] - X[i]\n",
        "      N = np.dot(M, M.T)\n",
        "      N = N + np.identity(N.shape[0]) * 1e-4\n",
        "      w_i = np.linalg.solve(N, np.ones(N.shape[0]))\n",
        "      s = w_i.sum()\n",
        "      w_i = w_i / s\n",
        "      for j in range(n_neig):\n",
        "        W[i,indices[i][j]] = w_i[j]\n",
        "  return W\n",
        "\n",
        "\n",
        "# Computes the loss\n",
        "def compute_loss(X,W, i):\n",
        "  predicted = np.dot(W[i],X)\n",
        "  return np.linalg.norm(predicted-X[i])\n",
        "\n",
        "#testing functions\n",
        "\n",
        "dataset_size = 40\n",
        "vector_dim = 1500\n",
        "X = np.random.rand(dataset_size,vector_dim)\n",
        "X[:,-1] = 0\n",
        "X[-1,:] = np.random.rand(1,vector_dim)\n",
        "X[-1,-1] = 1\n",
        "X = X * 15000\n",
        "W_sol = compute_weights(X,dataset_size, vector_dim, 4)\n",
        "\n",
        "for n in range(dataset_size):\n",
        "  val = compute_loss(X,W_sol,n)\n",
        "  print(val)\n",
        "  print(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVKhrQQ150oa",
        "outputId": "92adcafd-d6ba-4a3b-a41c-78516faaa356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.266700782914898e-10\n",
            " \n",
            "5.272597585521529e-10\n",
            " \n",
            "5.381592023847141e-10\n",
            " \n",
            "5.384207979744263e-10\n",
            " \n",
            "5.228351265559139e-10\n",
            " \n",
            "5.247723138385663e-10\n",
            " \n",
            "5.703139784190434e-10\n",
            " \n",
            "5.247466289702294e-10\n",
            " \n",
            "5.214223837587664e-10\n",
            " \n",
            "5.330433557792088e-10\n",
            " \n",
            "5.229781855433636e-10\n",
            " \n",
            "5.262757011099447e-10\n",
            " \n",
            "5.374461502521197e-10\n",
            " \n",
            "5.279522486725361e-10\n",
            " \n",
            "5.358084585632211e-10\n",
            " \n",
            "5.263712580595411e-10\n",
            " \n",
            "5.268596959680681e-10\n",
            " \n",
            "5.247431210053167e-10\n",
            " \n",
            "5.285211570987014e-10\n",
            " \n",
            "5.346079065932145e-10\n",
            " \n",
            "5.351814229736999e-10\n",
            " \n",
            "5.272140031546506e-10\n",
            " \n",
            "5.239316775605073e-10\n",
            " \n",
            "5.373070872900222e-10\n",
            " \n",
            "5.273577276245402e-10\n",
            " \n",
            "5.361498123640529e-10\n",
            " \n",
            "5.229993101777679e-10\n",
            " \n",
            "5.297988939296607e-10\n",
            " \n",
            "5.189664119388021e-10\n",
            " \n",
            "5.330862123513542e-10\n",
            " \n",
            "5.381641451459462e-10\n",
            " \n",
            "5.264394305676166e-10\n",
            " \n",
            "5.239540719804781e-10\n",
            " \n",
            "5.250244352257774e-10\n",
            " \n",
            "5.23230425400984e-10\n",
            " \n",
            "5.272470771334919e-10\n",
            " \n",
            "5.27117893433508e-10\n",
            " \n",
            "5.235347919216579e-10\n",
            " \n",
            "5.291022001103917e-10\n",
            " \n",
            "5.200552425894505e-10\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attempted to run a test on a few points lying on the subspace $R^{n-1}$, and one point with the last coordinate $1$. The algorithm was unable to distinguish between the points on the lower dimensional subspace and the spike."
      ],
      "metadata": {
        "id": "ldKzIYCvz750"
      }
    }
  ]
}